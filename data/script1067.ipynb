{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "199705d3-019d-adf9-4f52-1620daa3688f"
      },
      "source": [
        "I've noticed that all types of features contain 64 samples. So it would be good to split them into different dimensions and after it we will have input shape (64, 3) for one sample. \n",
        "\n",
        "The simple network described below gives 96% - 98% (depends on random initialization) accuracy on validation set with size 0.1 and  0.05467 of score in Kaggle competition on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8c05420f-90c5-82ff-5a69-29ed12c8112d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "\n",
        "def encode(train, test):\n",
        "    label_encoder = LabelEncoder().fit(train.species)\n",
        "    labels = label_encoder.transform(train.species)\n",
        "    classes = list(label_encoder.classes_)\n",
        "\n",
        "    train = train.drop(['species', 'id'], axis=1)\n",
        "    test = test.drop('id', axis=1)\n",
        "\n",
        "    return train, labels, test, classes\n",
        "\n",
        "train, labels, test, classes = encode(train, test)\n",
        "\n",
        "# standardize train features\n",
        "scaler = StandardScaler().fit(train.values)\n",
        "scaled_train = scaler.transform(train.values)\n",
        "\n",
        "# split train data into train and validation\n",
        "sss = StratifiedShuffleSplit(test_size=0.1, random_state=23)\n",
        "for train_index, valid_index in sss.split(scaled_train, labels):\n",
        "    X_train, X_valid = scaled_train[train_index], scaled_train[valid_index]\n",
        "    y_train, y_valid = labels[train_index], labels[valid_index]\n",
        "    \n",
        "\n",
        "nb_features = 64 # number of features per features type (shape, texture, margin)   \n",
        "nb_class = len(classes)\n",
        "\n",
        "# reshape train data\n",
        "X_train_r = np.zeros((len(X_train), nb_features, 3))\n",
        "X_train_r[:, :, 0] = X_train[:, :nb_features]\n",
        "X_train_r[:, :, 1] = X_train[:, nb_features:128]\n",
        "X_train_r[:, :, 2] = X_train[:, 128:]\n",
        "\n",
        "# reshape validation data\n",
        "X_valid_r = np.zeros((len(X_valid), nb_features, 3))\n",
        "X_valid_r[:, :, 0] = X_valid[:, :nb_features]\n",
        "X_valid_r[:, :, 1] = X_valid[:, nb_features:128]\n",
        "X_valid_r[:, :, 2] = X_valid[:, 128:]\n",
        "\n",
        "# Keras model with one Convolution1D layer\n",
        "# unfortunately more number of covnolutional layers, filters and filters lenght \n",
        "# don't give better accuracy\n",
        "model = Sequential()\n",
        "model.add(Convolution1D(nb_filter=512, filter_length=1, input_shape=(nb_features, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(nb_class))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "y_train = np_utils.to_categorical(y_train, nb_class)\n",
        "y_valid = np_utils.to_categorical(y_valid, nb_class)\n",
        "\n",
        "sgd = SGD(lr=0.01, nesterov=True, decay=1e-6, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "\n",
        "nb_epoch = 15\n",
        "model.fit(X_train_r, y_train, nb_epoch=nb_epoch, validation_data=(X_valid_r, y_valid), batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a0c84878-55d1-bfab-fd7c-091d67c2f58c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}