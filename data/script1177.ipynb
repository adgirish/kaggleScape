{"nbformat_minor": 1, "metadata": {"_change_revision": 0, "_is_fork": false, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "cells": [{"metadata": {"_uuid": "782b504aa323c70ff52739656a2e2e030ac3f2e7", "_cell_guid": "f9cc6c14-7246-e753-5df7-17b1fe660c64"}, "cell_type": "markdown", "source": ["# Intro\n", "\n", "I'm trying to learn the very basics with this exercise. My goal is to train a linear regression model with a subset of columns from this interesting dataset in order to predict the value of a used car.\n", "\n", "Any help or advice is welcome!!!\n", "\n", "### Changelist\n", "\n", "* left only random forest with gridsearchcv\n", "* rewritten all the notebook\n", "* added name length feature\n", "* better study on the data\n", "* used seaborn to plot\n", "* added random forest and xgboost algorithms"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "66acdc51b78b5ec3e00a1386c93888acd6192992", "_cell_guid": "05930236-246c-c7c2-3903-bd8523fcddff"}, "cell_type": "code", "source": ["import pandas as pd\n", "\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "from sklearn import datasets, linear_model, preprocessing, svm\n", "from sklearn.preprocessing import StandardScaler, Normalizer\n", "import math\n", "import matplotlib\n", "import seaborn as sns\n", "\n", "%matplotlib inline"], "execution_count": null}, {"metadata": {"_uuid": "1772de3005cac9188ed868615fb12e12d063250d", "_cell_guid": "7c971cfa-cbf1-76ef-2f6b-1aebe2e96b92"}, "cell_type": "markdown", "source": ["### Useful functions"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "74ba1ae7154860b616474a566e5274a6c4336d65", "_cell_guid": "dcf55a0f-510f-a2ee-108f-8fa71852a1cc"}, "cell_type": "code", "source": ["def category_values(dataframe, categories):\n", "    for c in categories:\n", "        print('\\n', dataframe.groupby(by=c)[c].count().sort_values(ascending=False))\n", "        print('Nulls: ', dataframe[c].isnull().sum())\n", "\n", "def plot_correlation_map( df ):\n", "    corr = df.corr()\n", "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n", "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n", "    _ = sns.heatmap(\n", "        corr, \n", "        cmap = cmap,\n", "        square=True, \n", "        cbar_kws={ 'shrink' : .9 }, \n", "        ax=ax, \n", "        annot = True, \n", "        annot_kws = { 'fontsize' : 12 }\n", "    )\n", "\n"], "execution_count": null}, {"metadata": {"_uuid": "b70a976892e7f98cc0ea51173662f89038745655", "_cell_guid": "ca01a4dc-9b43-196a-997d-21fc4e9118ba"}, "cell_type": "markdown", "source": ["# Preparing data\n", "\n", "## Reading from file\n", "\n", "Just reading the file and printing some lines."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "5dbf93047e52db1bd7f80baf092fa5ffbdc87f69", "_cell_guid": "a05292f8-9edf-4f27-2b61-4ca8d2aec1fc"}, "cell_type": "code", "source": ["df = pd.read_csv('../input/autos.csv', sep=',', header=0, encoding='cp1252')\n", "#df = pd.read_csv('autos.csv.gz', sep=',', header=0, compression='gzip',encoding='cp1252')\n", "df.sample(10)"], "execution_count": null}, {"metadata": {"_uuid": "4e6e0e21916dc9057dac6e88490fef1f332a4128", "_cell_guid": "e86791fa-ed15-1e1f-4f9f-e516c9f76457"}, "cell_type": "markdown", "source": ["Let's see some info from numeric fields"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "788db071fab57c35b3db6e4298ca87f8f0d2ad65", "_cell_guid": "561a67e9-6272-0f46-82a6-1c1a8ba2aebc"}, "cell_type": "code", "source": ["df.describe()"], "execution_count": null}, {"metadata": {"_uuid": "7691b4f5405e78545bd640fa9713fa2b2de40a95", "_cell_guid": "11a1ae56-5b63-8dac-ca19-625bd6595c6c"}, "cell_type": "markdown", "source": ["## Dropping some useless columns\n", "\n", "Some column can already be dropped."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "fce901fb0490627c754f596bda0bbe2016bd1609", "_cell_guid": "da977c90-95e1-69df-87dd-fdd5c8ea0f8e"}, "cell_type": "code", "source": ["print(df.seller.unique())\n", "print(df.offerType.unique())\n", "print(df.abtest.unique())\n", "print(df.nrOfPictures.unique())"], "execution_count": null}, {"metadata": {"_uuid": "c997f74963f6606047cb8a6d7ca2676f02e8736a", "_cell_guid": "13ff3216-fa18-8a0f-0648-61fc90e777ae"}, "cell_type": "markdown", "source": ["Seller has only one value, while offerType and abtest has no relevance for the analysis. So far, I still don't know how to use the `dateCrawled` column.\n", "\n", "Therefore I modify the dataframe dropping all those features.\n", "\n", "I remove `lastSeen`, `dateCreated` and `postalCode` as well as I don't think they will be useful for a price prediction."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "159858ce5411a8eaacf7d84bea006c3fead7c53a", "_cell_guid": "c0933c6b-9341-66a0-490d-ed3475e90f32"}, "cell_type": "code", "source": ["df.drop(['seller', 'offerType', 'abtest', 'dateCrawled', 'nrOfPictures', 'lastSeen', 'postalCode', 'dateCreated'], axis='columns', inplace=True)\n"], "execution_count": null}, {"metadata": {"_uuid": "2beb43c5f75b4963dfc4810d3e3fcbffb76878ef", "_cell_guid": "d1acc41d-86ff-9d61-68cc-e399b9cd2ebc"}, "cell_type": "markdown", "source": ["## Cleaning data\n", "\n", "Cleaning data from duplicates, NaNs and selecting reasonable ranges for columns\n"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "0a241a6a5b88ef7be2c5bf498a640f79f067988a", "_cell_guid": "a2e2e1ef-9202-e2e4-c754-5a744abc0b87"}, "cell_type": "code", "source": ["print(\"Too new: %d\" % df.loc[df.yearOfRegistration >= 2017].count()['name'])\n", "print(\"Too old: %d\" % df.loc[df.yearOfRegistration < 1950].count()['name'])\n", "print(\"Too cheap: %d\" % df.loc[df.price < 100].count()['name'])\n", "print(\"Too expensive: \" , df.loc[df.price > 150000].count()['name'])\n", "print(\"Too few km: \" , df.loc[df.kilometer < 5000].count()['name'])\n", "print(\"Too many km: \" , df.loc[df.kilometer > 200000].count()['name'])\n", "print(\"Too few PS: \" , df.loc[df.powerPS < 10].count()['name'])\n", "print(\"Too many PS: \" , df.loc[df.powerPS > 500].count()['name'])\n", "print(\"Fuel types: \" , df['fuelType'].unique())\n", "#print(\"Offer types: \" , df['offerType'].unique())\n", "#print(\"Sellers: \" , df['seller'].unique())\n", "print(\"Damages: \" , df['notRepairedDamage'].unique())\n", "#print(\"Pics: \" , df['nrOfPictures'].unique()) # nrOfPictures : number of pictures in the ad (unfortunately this field contains everywhere a 0 and is thus useless (bug in crawler!) )\n", "#print(\"Postale codes: \" , df['postalCode'].unique())\n", "print(\"Vehicle types: \" , df['vehicleType'].unique())\n", "print(\"Brands: \" , df['brand'].unique())\n", "\n", "# Cleaning data\n", "#valid_models = df.dropna()\n", "\n", "#### Removing the duplicates\n", "dedups = df.drop_duplicates(['name','price','vehicleType','yearOfRegistration'\n", "                         ,'gearbox','powerPS','model','kilometer','monthOfRegistration','fuelType'\n", "                         ,'notRepairedDamage'])\n", "\n", "#### Removing the outliers\n", "dedups = dedups[\n", "        (dedups.yearOfRegistration <= 2016) \n", "      & (dedups.yearOfRegistration >= 1950) \n", "      & (dedups.price >= 100) \n", "      & (dedups.price <= 150000) \n", "      & (dedups.powerPS >= 10) \n", "      & (dedups.powerPS <= 500)]\n", "\n", "print(\"-----------------\\nData kept for analisys: %d percent of the entire set\\n-----------------\" % (100 * dedups['name'].count() / df['name'].count()))\n"], "execution_count": null}, {"metadata": {"_uuid": "69f0f58445a0888637047577237308b19bfa1360", "_cell_guid": "588fec19-051c-86e4-ca61-d7aa199a22fa"}, "cell_type": "markdown", "source": ["## Working on the `null` values\n", "\n", "Checking if theree are NaNs to fix or drop"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "f742c98ca4727b300ae2e043369ddcb6059885dc", "_cell_guid": "790ed518-f40a-b1af-dd98-e5bfe9d27bab"}, "cell_type": "code", "source": ["dedups.isnull().sum()"], "execution_count": null}, {"metadata": {"_uuid": "adf667aa9ab18119f76e9785ae3a99b76765ea32", "_cell_guid": "12247e4b-b78e-604d-9355-046c7a656548"}, "cell_type": "markdown", "source": ["Some decisions to take for the nulls in the following fields: vehicleType (37422 nulls), gearbox (19803 nulls), model (20288 nulls), fuelType (33081 nulls), notRepairedDamage (70770 nulls).\n", "\n", "### `model`-`brand`-`vehicleType`\n", "If we have the `model` we could determine the `brand` and the `vehicleType` calculating the mode for the corresponding fields in the rest of the dataset. The opposite combinations are not true. So I think the actions should be:\n", "\n", "    | vehicleType | brand | model | Action\n", "    | ---           | ---     | ---     |\n", "    | null        |  null | [value] | Set the other fields\n", "    | null        | [value] | null  | Delete\n", "    | [value]       |  null | null  | Delete\n", "\n", "__So far, I'll drop all the NaNs in these 3 fields.__\n", "\n", "### `notRepairedDamage`\n", "Those with null `notRepairedDamage` field could be set to \"`not-declared`\" value for example.\n", "\n", "### `fuelType`\n", "Null `fuelType`s could be set to \"`not-declared`\" value again.\n", "\n", "### `gearbox`\n", "Null `fuelType`s could be set to \"`not-declared`\" value again.\n"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "dbb55dac9e6fb60680e3375e4b1f21f3fcc18bba", "_cell_guid": "b2db3fcb-4ef8-cf67-9e8a-eb10cd47dda3"}, "cell_type": "code", "source": ["dedups['notRepairedDamage'].fillna(value='not-declared', inplace=True)\n", "dedups['fuelType'].fillna(value='not-declared', inplace=True)\n", "dedups['gearbox'].fillna(value='not-declared', inplace=True)\n", "dedups['vehicleType'].fillna(value='not-declared', inplace=True)\n", "dedups['model'].fillna(value='not-declared', inplace=True)"], "execution_count": null}, {"metadata": {"_uuid": "7d017e25b9cf00ff9a6365e424640ae6e4845022", "_cell_guid": "2d57c352-d229-4e02-fcbe-bc9cbc2c5d87"}, "cell_type": "markdown", "source": ["Checking if all the nulls have been filled or dropped."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "2e0cb4600a320d26e74903293ebe3129890f35dd", "_cell_guid": "7e181607-6ada-fef6-8ca7-228acb4be82c"}, "cell_type": "code", "source": ["dedups.isnull().sum()"], "execution_count": null}, {"metadata": {"_uuid": "dd740cfebc0375ef63bf8e1ef48aabe4d0ff636a", "_cell_guid": "0c3a8615-6bbd-3ed1-8103-ce5facb7e1e9"}, "cell_type": "markdown", "source": ["OK, we're clear. Let's do some visualization now."]}, {"metadata": {"_uuid": "4a59d29449b07ea0651d4f680cdc4fab637e3704", "_cell_guid": "6ce21684-6568-8885-6acb-788d8ae014cb"}, "cell_type": "markdown", "source": ["## Visualizations\n", "### Categories distribution\n", "Let's see some charts to understand how data is distributed across the categories"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "c2bc946f4608a170f8e93401e3b03cf60b8acdfe", "_cell_guid": "a4bb7198-21bb-c55b-0b40-87a1b4862a89"}, "cell_type": "code", "source": ["categories = ['gearbox', 'model', 'brand', 'vehicleType', 'fuelType', 'notRepairedDamage']\n", "\n", "for i, c in enumerate(categories):\n", "    v = dedups[c].unique()\n", "    \n", "    g = dedups.groupby(by=c)[c].count().sort_values(ascending=False)\n", "    r = range(min(len(v), 5))\n", "\n", "    print( g.head())\n", "    plt.figure(figsize=(5,3))\n", "    plt.bar(r, g.head()) \n", "    #plt.xticks(r, v)\n", "    plt.xticks(r, g.index)\n", "    plt.show()"], "execution_count": null}, {"metadata": {"_uuid": "582403a8c279ed9bfdeb480f015d4ac18065b596", "_cell_guid": "a982dc85-18b2-25e1-ee41-aa25c97e1e98"}, "cell_type": "markdown", "source": ["### Feature engineering"]}, {"metadata": {"_uuid": "92c433cc3779e095ff59886cc96447f2da416483", "_cell_guid": "401b272f-04eb-941a-e638-9e4f9f1aba85"}, "cell_type": "markdown", "source": ["Adding the name length to see how much does a long description influence the price"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "80868acdf52f07cded33cc1b4724bb65e1223b23", "_cell_guid": "ea7d69c6-201a-15ec-e2a4-cb4ab61607fb"}, "cell_type": "code", "source": ["dedups['namelen'] = [min(70, len(n)) for n in dedups['name']]\n", "\n", "ax = sns.jointplot(x='namelen', \n", "                   y='price',\n", "                   data=dedups[['namelen','price']], \n", "#                   data=dedups[['namelen','price']][dedups['model']=='golf'], \n", "                    alpha=0.1, \n", "                    size=8)\n"], "execution_count": null}, {"metadata": {"_uuid": "7316fdcbaa41dc0b28e32d35c2e8d2608521a74d", "_cell_guid": "3f15aa3d-8147-4f5d-e841-5fd6039d6c65"}, "cell_type": "markdown", "source": ["It seems that a name length between 15 and 30 characters is better for the sale price. An explanation could be that a longer name includes more optionals and accessories and therefore the price is obviously higher.\n", "Very short and very long names do not work well."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "4ba478aeff47056263762ae08c328ea75d3093f2", "_cell_guid": "47dd0370-53ae-506f-94bf-06808a3cec74"}, "cell_type": "code", "source": ["labels = ['name', 'gearbox', 'notRepairedDamage', 'model', 'brand', 'fuelType', 'vehicleType']\n", "les = {}\n", "\n", "for l in labels:\n", "    les[l] = preprocessing.LabelEncoder()\n", "    les[l].fit(dedups[l])\n", "    tr = les[l].transform(dedups[l]) \n", "    dedups.loc[:, l + '_feat'] = pd.Series(tr, index=dedups.index)\n", "\n", "labeled = dedups[ ['price'\n", "                        ,'yearOfRegistration'\n", "                        ,'powerPS'\n", "                        ,'kilometer'\n", "                        ,'monthOfRegistration'\n", "                        , 'namelen'] \n", "                    + [x+\"_feat\" for x in labels]]\n"], "execution_count": null}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "d7d1f2d1742d4d86f3672a01233dc9e91b2a89e1", "_cell_guid": "40efb306-e78e-edda-4d48-27c4acd52d36"}, "cell_type": "code", "source": ["len(labeled['name_feat'].unique()) / len(labeled['name_feat'])"], "execution_count": null}, {"metadata": {"_uuid": "de59c069ee86d67a7af63e86872b878068e74ab5", "_cell_guid": "f786e8c0-846d-e305-a961-4080f06f9b5a"}, "cell_type": "markdown", "source": ["Labels for the name column account for 62% of the total. I think it's too much, so I remove the feature."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "4b90b47b8ebc67b12125298bcd74a49e8f264158", "_cell_guid": "0a138876-c44c-b852-b22f-33a99db264e2"}, "cell_type": "code", "source": ["labeled.drop(['name_feat'], axis='columns', inplace=True)"], "execution_count": null}, {"metadata": {"_uuid": "507949ef07cfc6e036471717bdb29c7a30ab4fe8", "_cell_guid": "d7d5bb97-3596-b1f5-0860-663e77b505e7"}, "cell_type": "markdown", "source": ["### Correlations\n", "Let's see how features are correlated each other and, more important, with the price."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "3656e7ff21cf20cc6a813a8d547906b17d72e913", "_cell_guid": "e24f19ae-6fcb-3521-fc49-edc87ff6b0fc"}, "cell_type": "code", "source": ["plot_correlation_map(labeled)\n", "labeled.corr()"], "execution_count": null}, {"metadata": {"_uuid": "8625bcf89d5515cf60e1d3140540f0aad0c02dc5", "_cell_guid": "c508e6ce-0234-8f81-5a35-6e2f5564c225"}, "cell_type": "markdown", "source": ["This is the list of the most influencing features for the price"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "6aa3482169cd5cb34a18b87baca5e6836ffd73e5", "_cell_guid": "d227b344-b47f-b61c-ebf9-4f3c369ee44b"}, "cell_type": "code", "source": ["labeled.corr().loc[:,'price'].abs().sort_values(ascending=False)[1:]"], "execution_count": null}, {"metadata": {"_uuid": "142617441d7039fadb3da4653bb8fac5f54f779e", "_cell_guid": "80d9e1f4-4094-fef8-2764-3026b424343f"}, "cell_type": "markdown", "source": ["I don't know why the model does not influence the car price more..."]}, {"metadata": {"_uuid": "68daef0107f6426da0da3a897e5f0d19d96c8caa", "_cell_guid": "740478a0-55d5-2c27-45d9-22b99592e3e6"}, "cell_type": "markdown", "source": ["# Playing with different models"]}, {"metadata": {"_uuid": "b59c731124b7f4535bf0dd16d0e9002107b4fe59", "_cell_guid": "eeb5c49e-62dd-e4d6-b786-652b7027c4d7"}, "cell_type": "markdown", "source": ["## Prepare data for training\n", "Here I split the dataset in train and validation data and tune the right-skewed sale price column."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "c36054e955d3679a8457bc3711e8a022d7b94fba", "_cell_guid": "f8fee99c-2557-7fdc-8036-6bb8194d4c3d"}, "cell_type": "code", "source": ["\n", "Y = labeled['price']\n", "X = labeled.drop(['price'], axis='columns', inplace=False)\n", "\n", "\n", "matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\n", "prices = pd.DataFrame({\"1. Before\":Y, \"2. After\":np.log1p(Y)})\n", "prices.hist()\n", "\n", "Y = np.log1p(Y)"], "execution_count": null}, {"metadata": {"_uuid": "88aa384905fd97fe9adbe2d95ec60b1371a2a120", "_cell_guid": "50a7f0e3-5cd7-a343-d3a7-64bb4e94702e"}, "cell_type": "markdown", "source": ["### Basic imports and functions\n", "\n", "Trying with some model from scikit learn: LinearRegression, LR with L2 regularization and others."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "3d5133f3fa46e003e93b4e348a155901c4ca27b7", "_cell_guid": "11a602ce-51a5-4993-d249-b704ecf2ca91"}, "cell_type": "code", "source": ["from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, Lasso, LassoCV, LassoLarsCV\n", "from sklearn.model_selection import cross_val_score, train_test_split\n", "\n", "def cv_rmse(model, x, y):\n", "    r = np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv = 5))\n", "    return r\n", "\n", "# Percent of the X array to use as training set. This implies that the rest will be test set\n", "test_size = .33\n", "\n", "#Split into train and validation\n", "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=test_size, random_state = 3)\n", "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n", "\n", "r = range(2003, 2017)\n", "km_year = 10000\n", "\n"], "execution_count": null}, {"metadata": {"_uuid": "7d3821599436c5d82a5840ed21c7d7d0b1ed1ad3", "_cell_guid": "9d5b0bbb-abb4-26e6-c15d-574bcef7f094"}, "cell_type": "markdown", "source": ["## Random forests\n", "\n", "I use the GridSearch to set the optimal parameteres for the regressor, then train the final model.\n", "\n", "I've removed the other parameters to quickly make this point pass online while I keep working on many parameters offline."]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "80ce69a3c3bd21d2cc588c761e7ec360d2dfb865", "_cell_guid": "f07da67d-f667-4221-0a3c-a04db92cdb05"}, "cell_type": "code", "source": ["from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.model_selection import GridSearchCV\n", "\n", "rf = RandomForestRegressor()\n", "\n", "param_grid = { \"criterion\" : [\"mse\"]\n", "              , \"min_samples_leaf\" : [3]\n", "              , \"min_samples_split\" : [3]\n", "              , \"max_depth\": [10]\n", "              , \"n_estimators\": [500]}\n", "\n", "gs = GridSearchCV(estimator=rf, param_grid=param_grid, cv=2, n_jobs=-1, verbose=1)\n", "gs = gs.fit(X_train, y_train)\n"], "execution_count": null}, {"metadata": {"_uuid": "4508145a4a08d280f82d298b81bd103571e3ea7e", "_cell_guid": "4573c56b-d1b2-e88d-42c7-fdc0b1f48452"}, "cell_type": "markdown", "source": ["#### Predicting samples"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "f0fcd264429993bae9e334aa59a35ed973793464", "_cell_guid": "b26b36a6-8696-d18e-bab1-6a12bffd0f3d"}, "cell_type": "code", "source": ["print(gs.best_score_)\n", "print(gs.best_params_)\n", " "], "execution_count": null}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "4b29dc9da4f00ee53c931258ce641db9f747847d", "_cell_guid": "99b5559a-5612-0d89-23ce-747cca5ecd0b"}, "cell_type": "code", "source": ["bp = gs.best_params_\n", "forest = RandomForestRegressor(criterion=bp['criterion'],\n", "                              min_samples_leaf=bp['min_samples_leaf'],\n", "                              min_samples_split=bp['min_samples_split'],\n", "                              max_depth=bp['max_depth'],\n", "                              n_estimators=bp['n_estimators'])\n", "forest.fit(X_train, y_train)\n", "# Explained variance score: 1 is perfect prediction\n", "print('Score: %.2f' % forest.score(X_val, y_val))\n"], "execution_count": null}, {"metadata": {"_uuid": "dc698167f175925a897d2bb7787a3686cd8b0ab1", "_cell_guid": "9bf7002d-d721-641e-e86e-d2313bf03f02"}, "cell_type": "markdown", "source": ["#### Predicting samples"]}, {"metadata": {"_uuid": "fa19118d4cb33bd19a55c8ab78cbf4c2ff622df8", "_cell_guid": "d6eed5d4-cb27-63b9-6439-7de2c2a00702"}, "cell_type": "markdown", "source": ["### Features importance"]}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "86bb25e724aeb12b78c8957b3f1228e45e6a7b0d", "_cell_guid": "3650d6bf-9200-ddca-1aa7-c922726e120a"}, "cell_type": "code", "source": ["importances = forest.feature_importances_\n", "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n", "             axis=0)\n", "indices = np.argsort(importances)[::-1]\n", "# Print the feature ranking\n", "print(\"Feature ranking:\")\n", "\n", "for f in range(X.shape[1]):\n", "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n", "\n", "print(X_train.columns.values)\n", "# Plot the feature importances of the forest\n", "plt.figure()\n", "plt.title(\"Feature importances\")\n", "plt.bar(range(X.shape[1]), importances[indices],\n", "       color=\"r\", yerr=std[indices], align=\"center\",tick_label = X_train.columns.values)\n", "plt.xticks(range(X.shape[1]), indices)\n", "plt.xlim([-1, X.shape[1]])\n", "plt.show()\n", "\n"], "execution_count": null}, {"metadata": {"_uuid": "f635b01111535458a9bb3761efe47e7fd6d9b72a", "_cell_guid": "632ada39-511d-61e3-0f3c-5dab7fed9d6c"}, "cell_type": "markdown", "source": ["# Conclusions\n", "\n", "I've tried to play with as much stuff as I could with this dataset in order to understand the very basic topics about:\n", "\n", "* data interpretation and selection\n", "* feature selection and labeling\n", "* data visualization\n", "* very rough ML algorithms application\n", "\n", "There's very much to improve both in how I managed all these steps and in the different outcomes of the predictions on the sale price. I'll experiment a bit more in the next few days, then I'll move on another dataset to learn more.\n"]}], "nbformat": 4}