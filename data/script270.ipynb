{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "668bf1c1-1623-dd32-9d87-88f1bae59b2a"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b4b10fd4-7bf5-a005-da4e-8880abb2eaa4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from IPython.display import display\n",
        "from scipy.stats import skew\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "# First look at the data\n",
        "print(train.shape)\n",
        "display(train.head())\n",
        "\n",
        "#NaN\n",
        "train.fillna(value=-999.0,inplace=True)\n",
        "test.fillna(value=-999.0,inplace=True)\n",
        "\n",
        "print('Type numeric:')\n",
        "list_feature_Nan = []\n",
        "for i in train.select_dtypes(exclude=['object']).columns:\n",
        "    if (train[i] == -999.0).astype(int).sum() > 0:\n",
        "        print(\"Feature: \", i, \"has\", round(((train[i] == -999.0).astype(int).sum()/1460)*100), \"% of NaN\")\n",
        "        list_feature_Nan.append(i)\n",
        "\n",
        "print('Type object:') \n",
        "\n",
        "for i in train.select_dtypes(include=['object']).columns:\n",
        "    if (train[i] == -999.0).astype(int).sum() > 0:\n",
        "        print(\"Feature: \", i, \"has\", round(((train[i] == -999.0).astype(int).sum()/1460)*100), \"% of NaN\") \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f0748a80-adcc-6614-25f6-91b032d56b51"
      },
      "outputs": [],
      "source": [
        "#Replace numeric feature by mean\n",
        "#train_replace_mean = train\n",
        "#test_replace_mean = test\n",
        "\n",
        "for i in list_feature_Nan:\n",
        "    train[i].replace(-999.0,train[i].mean(),inplace=True)\n",
        "    test[i].replace(-999.0,train[i].mean(),inplace=True)\n",
        "\n",
        "#train = train[np.log(train['SalePrice'])<13.5]    \n",
        "#train = train[np.log(train['SalePrice'])>10.60]  \n",
        "#train.reset_index(inplace=True,drop=True)\n",
        "\n",
        "y = np.array(pd.DataFrame(np.log(train.SalePrice)))\n",
        "y2 = np.array(pd.DataFrame(np.log(train.SalePrice)))\n",
        "\n",
        "mean_saleprice = pd.DataFrame(train.groupby(['GrLivArea'])['LotArea'].mean())\n",
        "mean_saleprice.columns = ['LotArea_bis']\n",
        "\n",
        "train.drop('SalePrice',axis=1,inplace=True)    \n",
        "\n",
        "#log transform skewed numeric features:\n",
        "all_data = all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition']))\n",
        "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
        "skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
        "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
        "skewed_feats = skewed_feats.index\n",
        "\n",
        "print(skewed_feats)\n",
        "\n",
        "train[skewed_feats] = np.log1p(train[skewed_feats])\n",
        "test[skewed_feats] = np.log1p(test[skewed_feats])\n",
        "\n",
        "train[skewed_feats] = train[skewed_feats].fillna(all_data[skewed_feats].mean())\n",
        "test[skewed_feats] = test[skewed_feats].fillna(all_data[skewed_feats].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "077f00b3-0dbc-f207-4f90-ac55034e3ec4"
      },
      "outputs": [],
      "source": [
        "#Label Encoder\n",
        "le = preprocessing.LabelEncoder()\n",
        "train_str = train.select_dtypes(include=['object'])\n",
        "test_str = test.select_dtypes(include=['object']) \n",
        "display(train_str.head())\n",
        "\n",
        "print(train_str.columns.values)\n",
        "\n",
        "train.drop(train_str.columns.values,axis=1,inplace=True)\n",
        "test.drop(train_str.columns.values,axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2908a1fe-e11c-b3ed-aed4-cc0c804b0109"
      },
      "outputs": [],
      "source": [
        "train_str_dum = pd.get_dummies(train_str)\n",
        "test_str_dum = pd.get_dummies(test_str)\n",
        "\n",
        "columns_dum = list(set(train_str_dum) & set(test_str_dum))\n",
        "\n",
        "train_str_dum = train_str_dum[columns_dum]\n",
        "test_str_dum = test_str_dum[columns_dum]\n",
        "\n",
        "#New train and New test\n",
        "train_flo = train.select_dtypes(exclude=['object'])\n",
        "test_flo = test.select_dtypes(exclude=['object']) \n",
        "\n",
        "new_train = pd.merge(train_flo,train_str_dum,left_index=True,right_index=True)\n",
        "new_test = pd.merge(test_flo,test_str_dum,left_index=True,right_index=True)\n",
        "\n",
        "display(new_train.head())\n",
        "print(new_train.columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "31666b8c-f4ca-249c-98d0-613222bc5aa6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.cross_validation import KFold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD, SparsePCA\n",
        "\n",
        "#train_clf = new_train.drop('SalePrice',axis=1)\n",
        "train_clf = new_train.copy()\n",
        "train_clf.drop('Id',axis=1,inplace=True)\n",
        "\n",
        "index = pd.DataFrame(test.Id,columns = ['Id'])\n",
        "test_clf = new_test.drop('Id',axis=1)\n",
        "\n",
        "train_clf2 = train_clf.drop(['LotFrontage','MasVnrArea','GarageYrBlt'],axis=1)\n",
        "train_clf2 = pd.merge(train_clf2,train[['LotFrontage','MasVnrArea','GarageYrBlt']],left_index=True,right_index=True)\n",
        "train_clf2['tot_sf'] = train_clf2['TotalBsmtSF'] + train_clf2['GrLivArea']\n",
        "train_clf2['ratio_fl'] = train_clf2['2ndFlrSF'] / train_clf2['1stFlrSF'] \n",
        "train_clf2['garage_ex'] = (train_clf2['GarageQual_Gd'] + train_clf2['GarageQual_TA'] + train_clf2['GarageQual_Fa'] + train_clf2['GarageQual_Po']) * (train_clf2['GarageCond_Ex'])\n",
        "\n",
        "clus = KernelPCA(n_components = 25)\n",
        "train_clf2_pca = clus.fit_transform(train_clf2)\n",
        "train_clf3 = pd.merge(train_clf2,pd.DataFrame(train_clf2_pca),left_index=True,right_index=True)\n",
        "\n",
        "test_clf2 = test_clf.drop(['LotFrontage','MasVnrArea','GarageYrBlt'],axis=1)\n",
        "test_clf2 = pd.merge(test_clf2,test[['LotFrontage','MasVnrArea','GarageYrBlt']],left_index=True,right_index=True)\n",
        "test_clf2['tot_sf'] = test_clf2['TotalBsmtSF'] + test_clf2['GrLivArea']\n",
        "test_clf2['ratio_fl'] = test_clf2['2ndFlrSF']  / test_clf2['1stFlrSF']\n",
        "test_clf2['garage_ex'] = (test_clf2['GarageQual_Gd'] + test_clf2['GarageQual_TA'] + test_clf2['GarageQual_Fa'] + test_clf2['GarageQual_Po']) * (test_clf2['GarageCond_Ex'])\n",
        "\n",
        "test_clf3 = pd.merge(test_clf2,pd.DataFrame(clus.transform(test_clf2)),left_index=True,right_index=True)\n",
        "\n",
        "index = pd.DataFrame(test.Id,columns = ['Id'])\n",
        "\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "def score(clf, train_np, random_state, folds, target, length):\n",
        "    kf = KFold(n = length , n_folds=folds, shuffle = True, random_state = random_state)\n",
        "    for itrain, itest in kf:\n",
        "        Xtr, Xte = train_np[itrain], train_np[itest]\n",
        "        ytr, yte = target[itrain], target[itest]\n",
        "        clf.fit(Xtr, ytr.ravel())\n",
        "        pred = pd.DataFrame(clf.predict(Xte))\n",
        "        return rmse(yte, pred)\n",
        "    return rmse(y, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da8c3734-c95f-c904-d620-76a1372af9ee"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "\n",
        "#Delete z_score > 4 feature = GrLivArea\n",
        "#z_score = pd.DataFrame(stats.zscore(pd.DataFrame(y), axis=1))\n",
        "#z_score.columns = train_clf3.columns\n",
        "#print(z_score[z_score['GrLivArea']>4].index)\n",
        "\n",
        "#train_clf4 = train_clf3.drop(train_clf3.index[z_score[z_score['GrLivArea']>4].index])\n",
        "#y_bis =  pd.DataFrame(y).drop(pd.DataFrame(y).index[z_score[z_score['GrLivArea']>4].index])\n",
        "#y_bis_array = np.array(y_bis)\n",
        "train_clf4 = train_clf3.copy()\n",
        "#train_clf4['LotArea'] = np.sqrt(train_clf3['LotArea'])\n",
        "\n",
        "#train_clf5 = train_clf3.copy()\n",
        "#train_clf5['LotArea'] = np.log1p(train_clf3['LotArea'])\n",
        "\n",
        "test_clf4 = test_clf3.copy()\n",
        "#test_clf4['LotArea'] = np.sqrt(test_clf3['LotArea'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed9fae0f-7af4-3609-9f93-028e76946216"
      },
      "outputs": [],
      "source": [
        "for i in train_clf4.select_dtypes(include=['float64']).columns:\n",
        "    if train_clf4[i].min() > test_clf4[i].min():\n",
        "        train_clf4[i][train_clf4[i]==train_clf4[i].min()] = test_clf4[i].min()\n",
        "\n",
        "for i in train_clf4.select_dtypes(include=['float64']).columns:\n",
        "    if train_clf4[i].min() < test_clf4[i].min():\n",
        "        test_clf4[i][test_clf4[i]==test_clf4[i].min()] = train_clf4[i].min()     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eb7d8937-a54b-18c1-e896-b20d20a8e3ee"
      },
      "outputs": [],
      "source": [
        "for i in train_clf4.select_dtypes(include=['float64']).columns:\n",
        "    if train_clf4[i].max() > test_clf4[i].max():\n",
        "        test_clf4[i][test_clf4[i]==test_clf4[i].max()] = train_clf4[i].max()\n",
        "\n",
        "for i in train_clf4.select_dtypes(include=['float64']).columns:\n",
        "    if train_clf4[i].max() < test_clf4[i].max():\n",
        "        train_clf4[i][train_clf4[i]==train_clf4[i].max()] = test_clf4[i].max()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f32a5774-a24e-b626-95cb-7b724ee1235b"
      },
      "outputs": [],
      "source": [
        "def put_corr_feat(data,test,target):\n",
        "    data_new = data.copy()\n",
        "    test_new = test.copy()\n",
        "    for i in data.columns:\n",
        "        corr1=stats.pearsonr(np.array(pd.DataFrame(data[i])),target)[0]\n",
        "        corr2=stats.pearsonr(np.array(pd.DataFrame(data[i]**3)),target)[0]\n",
        "        if abs(corr2)>abs(corr1)*1.2:\n",
        "            print(i)\n",
        "            #print(data[i])\n",
        "            #print(abs(corr2),'VS',abs(corr1))\n",
        "            data_new[i]=np.array(data_new[i])**3\n",
        "            #print(data_new[i])\n",
        "            test_new[i]=np.array(test_new[i])**3\n",
        "    return data_new, test_new\n",
        "    \n",
        "data1, test1 = put_corr_feat(train_clf3,test_clf3,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5bbc93c8-4c9c-f982-28ee-8b39b2189dc2"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "from sklearn.linear_model import Ridge, LassoCV\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.linear_model import LarsCV\n",
        "\n",
        "def frange(start, stop, step):\n",
        "    i = start\n",
        "    a = []\n",
        "    while i < stop:\n",
        "        yield i\n",
        "        a.append(i)\n",
        "        i += step\n",
        "    return a\n",
        "\n",
        "#Cs1 = list(range(2,10,1))\n",
        "#Cs1 = [15,16,17,18,19,20,21]\n",
        "Cs1 = [5000,6000,7000,8000,9000,10000]\n",
        "#Cs1 = [0.6,0.7,0.75,0.8,0.85,0.9]\n",
        "res = []\n",
        "res1 = []\n",
        "res2 = []\n",
        "res3 = []\n",
        "res4 = []\n",
        "Cs3=[]\n",
        "\n",
        "train_np = np.array(train_clf3)\n",
        "#train_np2 = np.array(data1)\n",
        "robust_scaler = RobustScaler()\n",
        "\n",
        "#for C in Cs1:\n",
        "#    res1.append(score(xgb.XGBRegressor(n_estimators = C, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n",
        "#    res2.append(score(xgb.XGBRegressor(n_estimators = C, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n",
        "    #res3.append(score(LassoCV(alphas = [C, 1, 0.1, 0.01, 0.001, 0.0005]), train_np, random_state = 0, folds = 7, target = y , length = 1450))\n",
        "    #res4.append(score(LassoCV(alphas = [C, 1, 0.1, 0.01, 0.001, 0.0005]), train_np2, random_state = 0, folds = 7, target = y , length = 1450))\n",
        "    #res1.append(score(xgb.XGBRegressor(n_estimators = 100, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),SelectKBest(f_regression, k = 268).fit_transform(train_np,y),random_state = 0, folds = 7, target = y , length = 1450))\n",
        "    #res2.append(score(GradientBoostingRegressor(n_estimators = 100,learning_rate=0.005, max_depth = 3, min_samples_split=800,min_samples_leaf = 40,max_features=230,subsample = 0.85,random_state = 0),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n",
        "    #res3.append(score(xgb.XGBRegressor(n_estimators = C, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n",
        "    #res3.append(score(MLPRegressor(hidden_layer_sizes=(800, ), activation = 'logistic', random_state = 0),train_np,random_state = 0, folds = C, target = y , length = 1450))\n",
        "    #res4.append(score(ExtraTreesRegressor(n_estimators = 700,  max_depth = 12, min_samples_leaf = 5, random_state = 0),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n",
        "    #res2.append(score(xgb.XGBRegressor(n_estimators = C, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),SelectKBest(f_regression, k = 268).fit_transform(train_np,y),random_state = 0, folds = 7))\n",
        "#     res.append(score(LassoCV(alphas = [ 1, 0.1, 0.01, 0.001, 0.0005]),SelectKBest(f_regression, k = C).fit_transform(train_clf3,y),random_state = 0, folds = 7))\n",
        "#     res2.append(score(LassoCV(alphas = [ 1, 0.1, 0.01, 0.001, 0.0005]),train_clf3,random_state = 0, folds = 7))\n",
        "     #res2.append(score(LassoCV(alphas = [ 1, 0.1, 0.001, 0.0005]),SelectKBest(f_regression, k = 268).fit_transform(train_clf3,y),random_state = 0, folds = 7))\n",
        "#for C in Cs2:    \n",
        "#    res2.append(score(AdaBoostRegressor(n_estimators = C, random_state = 42, learning_rate = 0.01, base_estimator = xgb.XGBRegressor(max_depth = 8, seed = 0)),train_np,random_state = 0, folds = 10))\n",
        "\n",
        "#p1, = plt.plot(Cs1, res1,'r-o',label=\"V1\")\n",
        "#p2, = plt.plot(Cs1, res2,'b-o',label=\"V2\")\n",
        "#p3, = plt.plot(Cs1, res3,'g-o',label=\"V3\")\n",
        "#p4, = plt.plot(Cs1, res4,'y-o',label=\"V4\")\n",
        "#plt.legend([p1,p2])\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b59d40b1-3d27-0207-d099-783490d2bbe2"
      },
      "outputs": [],
      "source": [
        "print(res)\n",
        "print(res2)\n",
        "print(res3)\n",
        "print(res4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "984a0060-f59d-91d4-7fe3-4e00bc802e35"
      },
      "outputs": [],
      "source": [
        "plt.hist(y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5efe18c7-dfc7-bc12-8b95-c0b1d023dc7a"
      },
      "outputs": [],
      "source": [
        "plt.boxplot(y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "914ade60-fba3-06af-02e4-1cff44caea70"
      },
      "outputs": [],
      "source": [
        "index = pd.DataFrame(test.Id,columns = ['Id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0b1a4c4e-c20d-079a-414a-8c212d50cca6"
      },
      "outputs": [],
      "source": [
        "## import libraries\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cross_validation import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "\n",
        "ntrain = train_clf4.shape[0]\n",
        "sparse_data = []\n",
        "tr_te = pd.concat((train_clf4, test_clf4), axis = 0)\n",
        "\n",
        "tmp = csr_matrix(tr_te)\n",
        "sparse_data.append(tmp)\n",
        "\n",
        "xtr_te = hstack(sparse_data, format = 'csr')\n",
        "xtrain = xtr_te[:ntrain, :]\n",
        "xtest = xtr_te[ntrain:, :]\n",
        "\n",
        "def batch_generator(X, y, batch_size, shuffle):\n",
        "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
        "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
        "    counter = 0\n",
        "    sample_index = np.arange(X.shape[0])\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "    while True:\n",
        "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
        "        X_batch = X[batch_index,:].toarray()\n",
        "        y_batch = y[batch_index]\n",
        "        counter += 1\n",
        "        yield X_batch, y_batch\n",
        "        if (counter == number_of_batches):\n",
        "            if shuffle:\n",
        "                np.random.shuffle(sample_index)\n",
        "            counter = 0\n",
        "\n",
        "def batch_generatorp(X, batch_size, shuffle):\n",
        "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
        "    counter = 0\n",
        "    sample_index = np.arange(X.shape[0])\n",
        "    while True:\n",
        "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
        "        X_batch = X[batch_index, :].toarray()\n",
        "        counter += 1\n",
        "        yield X_batch\n",
        "        if (counter == number_of_batches):\n",
        "            counter = 0\n",
        "def nn_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(10, input_dim = xtrain.shape[1], init = 'he_normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(5, init = 'he_normal'))\n",
        "    model.add(PReLU())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, init = 'he_normal'))\n",
        "    model.compile(loss = 'mse', optimizer = 'adadelta')\n",
        "    return(model)\n",
        "\n",
        "## cv-folds\n",
        "nfolds = 2\n",
        "folds = KFold(len(y), n_folds = nfolds, shuffle = True, random_state = 111)\n",
        "\n",
        "## train models\n",
        "i = 0\n",
        "nbags = 2\n",
        "nepochs = 1\n",
        "pred_oob = np.zeros(xtrain.shape[0])\n",
        "pred_test = np.zeros(xtest.shape[0])\n",
        "def model():\n",
        "    for (inTr, inTe) in folds:\n",
        "        xtr = xtrain[inTr]\n",
        "        ytr = y[inTr]\n",
        "        xte = xtrain[inTe]\n",
        "        yte = y[inTe]\n",
        "        pred = np.zeros(xte.shape[0])\n",
        "        for j in range(nbags):\n",
        "            model = nn_model()\n",
        "            fit = model.fit_generator(generator = batch_generator(xtr, ytr, 64, True),\n",
        "                                      nb_epoch = nepochs,\n",
        "                                      samples_per_epoch = xtr.shape[0],\n",
        "                                      verbose = 0)\n",
        "            pred += model.predict_generator(generator = batch_generatorp(xte, 50, False), val_samples = xte.shape[0])[:,0]\n",
        "            score = rmse(np.exp(yte), np.exp(pred))\n",
        "            print(score)\n",
        "            pred_test += model.predict_generator(generator = batch_generatorp(xtest, 25, False), val_samples = xtest.shape[0])[:,0]\n",
        "        pred /= nbags\n",
        "        pred_oob[inTe] = pred\n",
        "        #score = rmse(yte, pred)\n",
        "        i += 1\n",
        "        #print('Fold ', i, '- rmse:', score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b60eb19f-7987-7187-bd14-a4488e718bbf"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "#clf = LassoCV(alphas = [ 1, 0.1, 0.001, 0.0005], max_iter = 1000)\n",
        "clf = xgb.XGBRegressor(n_estimators = 5000, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 )\n",
        "clf2 = Ridge(alpha=21)\n",
        "clf3 = LassoCV(alphas = [1, 0.1, 0.001, 0.0005])\n",
        "#clf4 = GradientBoostingRegressor(n_estimators = 6000,learning_rate=0.005, max_depth = 3, min_samples_split=800,min_samples_leaf = 40,max_features=230,subsample = 0.85,random_state = 0)\n",
        "#clf5 = RandomForestRegressor(n_estimators = 750, max_depth = 8, min_samples_leaf = 2, random_state = 0)\n",
        "Select = SelectKBest(f_regression, k = 268)\n",
        "\n",
        "train_fin = np.array(Select.fit_transform(train_clf4,y))\n",
        "test_fin = np.array(Select.transform(test_clf4))\n",
        "\n",
        "train_fin_ridge = np.array(robust_scaler.fit_transform(train_clf3))\n",
        "test_fin_ridge = np.array(robust_scaler.transform(test_clf3))\n",
        "\n",
        "kf1 = KFold(n = 1450 , n_folds=7, random_state=0, shuffle = True)\n",
        "#kf2 = KFold(n = 1450 , n_folds=39, random_state=0, shuffle = True)\n",
        "\n",
        "i = 0\n",
        "res1 = []\n",
        "res4 = []\n",
        "\n",
        "for itrain, itest in kf1:\n",
        "    i = i + 1\n",
        "    Xtr, Xte = train_fin[itrain], train_fin[itest]\n",
        "    ytr, yte = y[itrain], y[itest]\n",
        "    clf.fit(Xtr, ytr.ravel())\n",
        "    if i == 1:\n",
        "        pred1 = pd.DataFrame(clf.predict(test_fin))\n",
        "        print(\"Fold 1 :\", rmse(yte, pd.DataFrame(clf.predict(Xte))))\n",
        "        res1.append(rmse(yte, pd.DataFrame(clf.predict(Xte))))\n",
        "    if i > 1 :\n",
        "        pred1 = pred1 + pd.DataFrame(clf.predict(test_fin))\n",
        "        print(\"Fold \",i, \" :\", rmse(yte, pd.DataFrame(clf.predict(Xte))))\n",
        "        res1.append(rmse(yte, pd.DataFrame(clf.predict(Xte))))\n",
        "\n",
        "        \n",
        "#clf.fit(train_fin, y)\n",
        "#pred_1 = pd.DataFrame(clf.predict(test_fin))\n",
        "        \n",
        "clf2.fit(train_fin_ridge, y)\n",
        "pred2 = pd.DataFrame(clf2.predict(test_fin_ridge))\n",
        "        \n",
        "clf3.fit(train_fin_ridge, y)\n",
        "pred3 = pd.DataFrame(clf3.predict(test_fin_ridge))\n",
        "        \n",
        "pred_1 = pred1/7 \n",
        "\n",
        "\n",
        "for i in pred2[np.exp(pred2).values<1].index.values:\n",
        "    pred2[pred2.index == i] = pred_1[pred_1.index == i]\n",
        "\n",
        "\n",
        "for i in pred2[pred2.values>12].index.values:\n",
        "    pred2[pred2.index == i] = pred_1[pred_1.index == i]\n",
        "    \n",
        "print(np.mean(res1))\n",
        "\n",
        "pred_1 = (pred_1+2*pred2+pred3)/4\n",
        "pred_2 = (pred_1+pred3)/2\n",
        "pred_xg = pred_1\n",
        "\n",
        "pred_1.columns = ['SalePrice']\n",
        "pred_2.columns = ['SalePrice']\n",
        "pred_xg.columns = ['SalePrice']\n",
        "pred3.columns = ['SalePrice']\n",
        "\n",
        "pred_final_1 = pd.DataFrame(np.exp(pred_1), index = new_test.index, columns = ['SalePrice'])\n",
        "pred_final_2 = pd.DataFrame(np.exp(pred_2), index = new_test.index, columns = ['SalePrice'])\n",
        "pred_Lasso = pd.DataFrame(np.exp(pred3), index = new_test.index, columns = ['SalePrice'])\n",
        "pred_xg = pd.DataFrame(np.exp(pred_xg), index = new_test.index, columns = ['SalePrice'])\n",
        "\n",
        "#replace by mean\n",
        "for i in pred_final_1[pred_final_1.values<100].index.values:\n",
        "    pred_final_1[pred_final_1.index == i] = 180921.1959\n",
        "    \n",
        "pred_final = pred_final_1\n",
        "\n",
        "pred_submit = pd.merge(index,pred_final,left_index=True,right_index=True)\n",
        "pred_Lasso_submit = pd.merge(index,pred_Lasso,left_index=True,right_index=True)\n",
        "pred_xg_submit = pd.merge(index,pred_xg,left_index=True,right_index=True)\n",
        "pred_xg_lasso_submit = pd.merge(index,pred_final_2,left_index=True,right_index=True)\n",
        "\n",
        "pred_submit.to_csv('XG_Ridge_Lasso.csv',index=False)\n",
        "pred_Lasso_submit.to_csv('Lasso.csv',index=False)\n",
        "pred_xg_submit.to_csv('XGB.csv',index=False)\n",
        "pred_xg_lasso_submit.to_csv('XGB_Lasso.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}