{"cells":[
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "# Python for Padawans\n\nThis tutorial will go throughthe basic data wrangling workflow I'm sure you all love to hate, in Python! \nFYI: I come from a R background (aka I'm not a proper programmer) so if you see any formatting issues please cut me a bit of slack. \n\n**The aim for this post is to show people how to easily move their R workflows to Python (especially pandas/scikit)**\n\nOne thing I especially like is how consistent all the functions are. You don't need to switch up style like you have to when you move from base R to dplyr etc. \n|\nAnd also, it's apparently much easier to push code to production using Python than R. So there's that. \n\n### 1. Reading in libraries"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "%matplotlib inline\nimport os\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport math"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "#### Don't forget that %matplotlib function. Otherwise your graphs will pop up in separate windows and stop the execution of further cells. And nobody got time for that.\n\n### 2. Reading in data"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "data = pd.read_csv('../input/loan.csv', low_memory=False)\ndata.drop(['id', 'member_id', 'emp_title'], axis=1, inplace=True)\n\ndata.replace('n/a', np.nan,inplace=True)\ndata.emp_length.fillna(value=0,inplace=True)\n\ndata['emp_length'].replace(to_replace='[^0-9]+', value='', inplace=True, regex=True)\ndata['emp_length'] = data['emp_length'].astype(int)\n\ndata['term'] = data['term'].apply(lambda x: x.lstrip())"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 3. Basic plotting using Seaborn\n\nNow let's make some pretty graphs. Coming from R I definitely prefer ggplot2 but the more I use Seaborn, the more I like it. If you kinda forget about adding \"+\" to your graphs and instead use the dot operator, it does essentially the same stuff.\n\n**And I've just found out that you can create your own style sheets to make life easier. Wahoo!**\n\nBut anyway, below I'll show you how to format a decent looking Seaborn graph, as well as how to summarise a given dataframe."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "import seaborn as sns\nimport matplotlib\n\ns = pd.value_counts(data['emp_length']).to_frame().reset_index()\ns.columns = ['type', 'count']\n\ndef emp_dur_graph(graph_title):\n\n    sns.set_style(\"whitegrid\")\n    ax = sns.barplot(y = \"count\", x = 'type', data=s)\n    ax.set(xlabel = '', ylabel = '', title = graph_title)\n    ax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n    _ = ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n    \nemp_dur_graph('Distribution of employment length for issued loans')"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 4. Using Seaborn stylesheets\n\nNow before we move on, we'll look at using style sheets to customize our graphs nice and quickly."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "import seaborn as sns\nimport matplotlib\n\nprint (plt.style.available)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Now you can see that we've got quite a few to play with. I'm going to focus on the following styles:\n\n- fivethirtyeight (because it's my fav website)\n- seaborn-notebook\n- ggplot\n- classic"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "import seaborn as sns\nimport matplotlib\n\nplt.style.use('fivethirtyeight')\nax = emp_dur_graph('Fivethirty eight style')"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "plt.style.use('seaborn-notebook')\nax = emp_dur_graph('Seaborn-notebook style')"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "plt.style.use('ggplot')\nax = emp_dur_graph('ggplot style')"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "plt.style.use('classic')\nax = emp_dur_graph('classic style')"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 5. Working with dates\n\nNow we want to looking at datetimes. Dates can be quite difficult to manipulate but it's worth the wait. Once they're formatted correctly life becomes much easier"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "import datetime\n\ndata.issue_d.fillna(value=np.nan,inplace=True)\nissue_d_todate = pd.to_datetime(data.issue_d)\ndata.issue_d = pd.Series(data.issue_d).str.replace('-2015', '')\ndata.emp_length.fillna(value=np.nan,inplace=True)\n\ndata.drop(['loan_status'],1, inplace=True)\n\ndata.drop(['pymnt_plan','url','desc','title' ],1, inplace=True)\n\ndata.earliest_cr_line = pd.to_datetime(data.earliest_cr_line)\nimport datetime as dt\ndata['earliest_cr_line_year'] = data['earliest_cr_line'].dt.year"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 6. Making faceted graphs using Seaborn\n\nNow I'll show you how you can build on the above data frame summaries as well as make some facet graphs."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ns = pd.value_counts(data['earliest_cr_line']).to_frame().reset_index()\ns.columns = ['date', 'count']\n\ns['year'] = s['date'].dt.year\ns['month'] = s['date'].dt.month\n\nd = s[s['year'] > 2008]\n\nplt.rcParams.update(plt.rcParamsDefault)\nsns.set_style(\"whitegrid\")\n\ng = sns.FacetGrid(d, col=\"year\")\ng = g.map(sns.pointplot, \"month\", \"count\")\ng.set(xlabel = 'Month', ylabel = '')\naxes = plt.gca()\n_ = axes.set_ylim([0, d.year.max()])\nplt.tight_layout()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Now I want to show you how to easily drop columns that match a given pattern. Let's drop any column that includes \"mths\" in it."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "mths = [s for s in data.columns.values if \"mths\" in s]\nmths\n\ndata.drop(mths, axis=1, inplace=True)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 7. Using groupby to create summary graphs"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "group = data.groupby('grade').agg([np.mean])\nloan_amt_mean = group['loan_amnt'].reset_index()\n\nimport seaborn as sns\nimport matplotlib\n\nplt.style.use('fivethirtyeight')\n\nsns.set_style(\"whitegrid\")\nax = sns.barplot(y = \"mean\", x = 'grade', data=loan_amt_mean)\nax.set(xlabel = '', ylabel = '', title = 'Average amount loaned, by loan grade')\nax.get_yaxis().set_major_formatter(\nmatplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=0)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 8. More advanced groupby statements visualised with faceted graphs"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "filtered  = data[data['earliest_cr_line_year'] > 2008]\ngroup = filtered.groupby(['grade', 'earliest_cr_line_year']).agg([np.mean])\n\ngraph_df = group['int_rate'].reset_index()\n\nimport seaborn as sns\nimport matplotlib\n\nplt.style.use('fivethirtyeight')\nplt.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n\nsns.set_style(\"whitegrid\")\ng = sns.FacetGrid(graph_df, col=\"grade\", col_wrap = 2)\ng = g.map(sns.pointplot, \"earliest_cr_line_year\", \"mean\")\ng.set(xlabel = 'Year', ylabel = '')\naxes = plt.gca()\naxes.set_ylim([0, graph_df['mean'].max()])\n_ = plt.tight_layout()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 9. Treatment of missing values\nThis section is a toughie because there really is no correct answer. A pure data science/mining approach would test each of the approaches here using a CV split and include the most accurate treatment in their modelling pipeline.\nHere I have included the code for the following treatments:\n\n- Mean imputation\n- Median imputation\n- Algorithmic imputation\n\nI spent a large amount of time looking at 3. because I couldn't find anyone else who has implemented it, so I built it myself. In R it's very easy to use supervised learning techniques to impute missing values for a given variable (as shown here: https://www.kaggle.com/mrisdal/shelter-animal-outcomes/quick-dirty-randomforest) but sadly I couldn't find it done in Python."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#data['emp_length'].fillna(data['emp_length'].mean())\n#data['emp_length'].fillna(data['emp_length'].median())\n#data['emp_length'].fillna(data['earliest_cr_line_year'].median())\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf =  RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1)\n\ndata['emp_length'].replace(to_replace=0, value=np.nan, inplace=True, regex=True)\n\ncat_variables = ['term', 'purpose', 'grade']\ncolumns = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'grade', 'purpose', 'term']\n\ndef impute_missing_algo(df, target, cat_vars, cols, algo):\n\n    y = pd.DataFrame(df[target])\n    X = df[cols].copy()\n    X.drop(cat_vars, axis=1, inplace=True)\n\n    cat_vars = pd.get_dummies(df[cat_vars])\n\n    X = pd.concat([X, cat_vars], axis = 1)\n\n    y['null'] = y[target].isnull()\n    y['null'] = y.loc[:, target].isnull()\n    X['null'] = y[target].isnull()\n\n    y_missing = y[y['null'] == True]\n    y_notmissing = y[y['null'] == False]\n    X_missing = X[X['null'] == True]\n    X_notmissing = X[X['null'] == False]\n\n    y_missing.loc[:, target] = ''\n\n    dfs = [y_missing, y_notmissing, X_missing, X_notmissing]\n    \n    for df in dfs:\n        df.drop('null', inplace = True, axis = 1)\n\n    y_missing = y_missing.values.ravel(order='C')\n    y_notmissing = y_notmissing.values.ravel(order='C')\n    X_missing = X_missing.as_matrix()\n    X_notmissing = X_notmissing.as_matrix()\n    \n    algo.fit(X_notmissing, y_notmissing)\n    y_missing = algo.predict(X_missing)\n\n    y.loc[(y['null'] == True), target] = y_missing\n    y.loc[(y['null'] == False), target] = y_notmissing\n    \n    return(y[target])\n\ndata['emp_length'] = impute_missing_algo(data, 'emp_length', cat_variables, columns, rf)\ndata['earliest_cr_line_year'] = impute_missing_algo(data, 'earliest_cr_line_year', cat_variables, columns, rf)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 10. Running a simple classification model\nHere I take my cleaned variables (missing values have been imputed using random forests) and run a simple sklearn algo to classify the term of the loan.\nThis step in the analytics pipeline does take longer in Python than in R (as R handles factor variables out of the box while sklearn only accepts numeric features) but it isn't that hard.\nThis is just indicative though! A number of the variables are likely to introduce leakage to the prediction problem as they'll influence the term of the loan either directly or indirectly."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "y = data.term\n\ncols = ['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'int_rate', 'grade', 'emp_length', 'purpose', 'earliest_cr_line_year']\nX = pd.get_dummies(data[cols])\n\nfrom sklearn import preprocessing\n\ny = y.apply(lambda x: x.lstrip())\n\nle = preprocessing.LabelEncoder()\nle.fit(y)\n\ny = le.transform(y)\nX = X.as_matrix()\n\nfrom sklearn import linear_model\n\nlogistic = linear_model.LogisticRegression()\n\nlogistic.fit(X, y)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "### 11. Pipelining in sklearn\n\nIn this section I'll go through how you can combine multiple techniques (supervised an unsupervised) in a pipeline.\nThese can be useful for a number of reasons:\n\n- You can score the output of the whole pipeline\n- You can gridsearch for the whole pipeline making finding optimal parameters easier\n\nSo next we'll combine some a PCA (unsupervised) and Random Forests (supervised) to create a pipeline for modelling the data. \n\nIn addition to this I'll show you an easy way to grid search for the optimal hyper parameters."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "from sklearn import linear_model, decomposition\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\n\nrf = RandomForestClassifier(max_depth=5, max_features=1)\n\npca = decomposition.PCA()\npipe = Pipeline(steps=[('pca', pca), ('rf', rf)])\n\nn_comp = [3, 5]\nn_est = [10, 20]\n\nestimator = GridSearchCV(pipe,\n                         dict(pca__n_components=n_comp,\n                              rf__n_estimators=n_est))\n\nestimator.fit(X, y)"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}